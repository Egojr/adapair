{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd95be99-7224-43ea-ae18-944dc1917c0a",
   "metadata": {},
   "source": [
    "# Packages and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "585281dd-af68-440b-b099-306a72e7a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from sentence_transformers import models, losses, datasets, SentencesDataset\n",
    "from sentence_transformers import SentenceTransformer, util, InputExample\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from collections import OrderedDict\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from typing import Iterable, Dict\n",
    "\n",
    "from metrics import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ce57d-57e3-497f-8ec2-960cc02009d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify variables\n",
    "model_name = \"bert-base-uncased\"\n",
    "train_batch_size = 20\n",
    "max_seq_length = 250\n",
    "num_epochs = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = SentenceTransformer(model_name).to(device)\n",
    "\n",
    "# get embedding dimension\n",
    "num_features = model[1].word_embedding_dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238aaf80-4ba5-430d-bc01-fcea8ea38ee8",
   "metadata": {},
   "source": [
    "# Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c568a7d-93e4-4df8-8c1e-060246bd8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from huggingface\n",
    "dataset = load_dataset(\"multi_nli\")\n",
    "\n",
    "# and make dataset as dataframe for easier usage\n",
    "df = pd.DataFrame()\n",
    "df[\"premise\"] = dataset[\"train\"][\"premise\"]\n",
    "df[\"hypothesis\"] = dataset[\"train\"][\"hypothesis\"]\n",
    "df[\"genre\"] = dataset[\"train\"][\"genre\"]\n",
    "df[\"label\"] = dataset[\"train\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9acb9b7-174b-44dd-916c-28fcf6601925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tokenizer sends some warnings about the truncation strategy but all sentences are shorter than max_seq_length\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# Each different hierarchy needs a different label for AdaCos\n",
    "labels = []\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for i in tqdm(df.iterrows()):\n",
    "    if i[1][\"genre\"] == \"telephone\":\n",
    "        tmp = 0\n",
    "    elif i[1][\"genre\"] == \"government\":\n",
    "        tmp = 3\n",
    "    elif i[1][\"genre\"] == \"travel\":\n",
    "        tmp = 6\n",
    "    elif i[1][\"genre\"] == \"fiction\":\n",
    "        tmp = 9\n",
    "    elif i[1][\"genre\"] == \"slate\":\n",
    "        tmp = 12\n",
    "            \n",
    "    lab = int(i[1][\"label\"]) + tmp\n",
    "    \n",
    "    encoded_data = tokenizer(\n",
    "        i[1][\"premise\"],\n",
    "        i[1][\"hypothesis\"],\n",
    "        add_special_tokens = True, \n",
    "        return_attention_mask = True, \n",
    "        padding = \"max_length\", \n",
    "        max_length = max_seq_length, \n",
    "        return_tensors = 'pt',\n",
    "        truncation = True\n",
    "    )\n",
    "\n",
    "    input_ids.append(encoded_data['input_ids'])\n",
    "    attention_masks.append(encoded_data['attention_mask'])\n",
    "    labels.append(lab)\n",
    "    \n",
    "input_ids = torch.cat(input_ids)\n",
    "attention_masks = torch.cat(attention_masks)\n",
    "labels = torch.LongTensor(labels)\n",
    "\n",
    "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6696764-3102-4663-9d8f-667066076204",
   "metadata": {},
   "source": [
    "# Train AdaCos model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821d769-1f7c-47d9-81f4-c9bae6f820f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def train(train_loader, model, metrics_fc, criterion, optimizer, optimizer2):\n",
    "    losses = AverageMeter()\n",
    "    acc1s = AverageMeter()\n",
    "    \n",
    "    model.train()\n",
    "    metrics_fc.train()\n",
    "    \n",
    "    for i,batch in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = {\n",
    "            'input_ids': batch[0].to(device),\n",
    "            'attention_mask': batch[1].to(device),\n",
    "        }\n",
    "\n",
    "        feature = model(inputs)[\"sentence_embedding\"]\n",
    "        target = torch.LongTensor(batch[2]).to(device)\n",
    "        \n",
    "        output = metrics_fc(feature, target)\n",
    "        loss = criterion(output, target)\n",
    "        acc1, = accuracy(output, target, topk=(1,))\n",
    "        \n",
    "        losses.update(loss.item(), len(batch[0]))\n",
    "        acc1s.update(acc1.item(), len(batch[0]))\n",
    " \n",
    "        # compute gradient and do optimizing step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer2.step()\n",
    "\n",
    "    log = OrderedDict([\n",
    "        ('loss', losses.avg),\n",
    "        ('acc1', acc1s.avg),\n",
    "    ])\n",
    "            \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e148e4-383c-4b3b-bc55-cdee5c46636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the losses needed for adacos\n",
    "adacos = metrics.AdaCos(num_features, num_classes=15).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Prepare optimizers for model and adacos layers\n",
    "optimizer_m = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "optimizer = optim.SGD(adacos.parameters(), lr=1e-5, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"epoch{epoch+1}\")\n",
    "    train(train_dataloader, model, adacos, criterion, optimizer, optimizer_m)\n",
    "    \n",
    "    tmp = pd.Series([\n",
    "        epoch,\n",
    "        train_log['loss'],\n",
    "        train_log['acc1'],\n",
    "    ], index=['epoch', 'loss', 'acc1'])\n",
    "    print(tmp)\n",
    "\n",
    "model.save(f'model_adacos_mnli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef07f3-d13a-4df0-9961-c7739783f007",
   "metadata": {},
   "source": [
    "# Prepare data for pairwise training\n",
    "For clarity we separate the dataset preprocessing, but it can be done at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8fc4ce-fa4b-4a7f-93a7-74f676c490a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise cosine loss expects labels to be opposite for opposite classes (positive, negative)\n",
    "# for example for telephone the labels will be 1 for positive, -1 for negative\n",
    "# for government the labels will be 2 for positive and -2 for negative and so on\n",
    "# neutral classes are mapped to values whose absolute value is not shared by any other class\n",
    "train_examples = []\n",
    "for i in tqdm(df.iterrows()):\n",
    "    if i[1][\"genre\"] == \"telephone\":\n",
    "        if int(i[1][\"label\"]) != 1:\n",
    "            tmp = -(int(i[1][\"label\"]) - 1)\n",
    "        else:\n",
    "            tmp = int(i[1][\"label\"]) + 10\n",
    "    elif i[1][\"genre\"] == \"government\":\n",
    "        if int(i[1][\"label\"]) != 1:\n",
    "            tmp = -(int(i[1][\"label\"]) - 1) * 2\n",
    "        else:\n",
    "            tmp = int(i[1][\"label\"]) + 11\n",
    "    elif i[1][\"genre\"] == \"travel\":\n",
    "        if int(i[1][\"label\"]) != 1:\n",
    "            tmp = -(int(i[1][\"label\"]) - 1) * 3\n",
    "        else:\n",
    "            tmp = int(i[1][\"label\"]) + 12\n",
    "    elif i[1][\"genre\"] == \"fiction\":\n",
    "        if int(i[1][\"label\"]) != 1:\n",
    "            tmp = -(int(i[1][\"label\"]) - 1) * 4\n",
    "        else:\n",
    "            tmp = int(i[1][\"label\"]) + 13\n",
    "    elif i[1][\"genre\"] == \"slate\":\n",
    "        if int(i[1][\"label\"]) != 1:\n",
    "            tmp = -(int(i[1][\"label\"]) - 1) * 5\n",
    "        else:\n",
    "            tmp = int(i[1][\"label\"]) + 14\n",
    "    lab = tmp\n",
    "\n",
    "    input = i[1][\"premise\"] + \" \" + tokenizer.sep_token + \" \" + i[1][\"hypothesis\"]\n",
    "    train_examples.append(InputExample(texts=[input], label=lab))  \n",
    "\n",
    "train_dataset = SentencesDataset(train_examples, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b97f24-d959-4941-b18c-8a804540edae",
   "metadata": {},
   "source": [
    "# Train Pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "278df17d-5fa3-43ac-855a-f70543e471af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pairwise loss function\n",
    "class Pairwise_Cosine_Loss(nn.Module):\n",
    "    def __init__(self, model: SentenceTransformer, t=0.3):\n",
    "        super(Pairwise_Cosine_Loss, self).__init__()\n",
    "        self.sentence_embedder = model\n",
    "        self.t = t\n",
    "\n",
    "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
    "        rep = self.sentence_embedder(sentence_features[0])['sentence_embedding']\n",
    "        return self.pairwise_loss(labels, rep)\n",
    "\n",
    "    def pairwise_loss(self, labels, embeddings):\n",
    "        \"\"\"Build the pairwise loss over a batch of embeddings.\n",
    "        We generate all the pairs and average the loss.\n",
    "        Args:\n",
    "            labels: labels of the batch, of size (batch_size,)\n",
    "            embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        Returns:\n",
    "            Pairwise loss: scalar tensor containing the pairwise loss\n",
    "        \"\"\"\n",
    "        # Get the pairwise distance matrix\n",
    "        pairwise_dist = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "        \n",
    "        # mask for positive/neutral/negative split\n",
    "        mask_positive = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "        mask_negative = labels.unsqueeze(0) == -labels.unsqueeze(1)\n",
    "        mask_neutral = ~mask_negative & ~mask_positive\n",
    "        \n",
    "        # create distance objective matrix\n",
    "        objective = mask_positive.float() + (mask_negative.float() * -1)\n",
    "        # get errors\n",
    "        pairwise_loss = pairwise_dist - objective\n",
    "        pairwise_loss = torch.abs(pairwise_loss)\n",
    "        # make losses null for neutral classes\n",
    "        pairwise_loss[(pairwise_loss < self.t) * mask_neutral] = 0\n",
    "        \n",
    "        # Get mean pairwise loss\n",
    "        pairwise_loss = pairwise_loss.sum() / (len(labels)**2)\n",
    "        \n",
    "        return pairwise_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4c759-30f2-49cd-a191-049f6e793389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the adacos model if needed\n",
    "# model = SentenceTransformer(f'model_adacos_mnli')\n",
    "\n",
    "# define the loss\n",
    "train_loss = Pairwise_Cosine_Loss(model)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          show_progress_bar=True,\n",
    "          optimizer_params={'lr': 1e-05}\n",
    "          )\n",
    "\n",
    "model.save(\"model_pairwise_mnli\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
